{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbbzxxY1FipS"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Ускоренный FHR vs Baseline — финальная версия (патчи безопасности + расширенный DSI анализ)\n",
        "\n",
        "Добавлено/исправлено:\n",
        "  • Безопасный tan (через sin/cos + clamp)\n",
        "  • Корректный unstable_ratio (по элементам)\n",
        "  • FFT анализа по гистограмме log(|w|) с Hann-окном\n",
        "  • Подсчёт SNR и bootstrap p-value пика в окне ожидаемой частоты\n",
        "  • По-слойный DSI-анализ (top-K слоёв), CSV + bar-график\n",
        "  • Сохранение картинок в weight_plots/\n",
        "  • Анализ редуцирован (подвыборка)\n",
        "  • Проверка NaN/Inf в градиентах и безопасный fallback\n",
        "  • Сохранение лучшей модели (checkpoints/)\n",
        "  • Поправка подписи частоты: 1/ln    (3) ≈ 0.9102\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "from typing import Dict, Tuple, Optional, List\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ========== 1) ускоренная среда ==========\n",
        "def setup_accelerated_environment(seed=1337, deterministic=False) -> Tuple[torch.device, int]:\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = deterministic\n",
        "    torch.backends.cudnn.benchmark = not deterministic\n",
        "\n",
        "    try:\n",
        "        num_workers = max(1, len(os.sched_getaffinity(0)) * 2)\n",
        "    except Exception:\n",
        "        num_workers = 4\n",
        "\n",
        "    print(f\"Ускоренный режим: {device}, workers={num_workers}\")\n",
        "    return device, num_workers\n",
        "\n",
        "\n",
        "DEVICE, NUM_WORKERS = setup_accelerated_environment()\n",
        "\n",
        "# создаём папки для артефактов\n",
        "os.makedirs(\"weight_plots\", exist_ok=True)\n",
        "os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "\n",
        "# ========== 2) логгер ==========\n",
        "class AcceleratedLogger:\n",
        "    def __init__(self, csv_path=\"fhr_results.csv\", update_freq=3):\n",
        "        self.history = []\n",
        "        self.csv_path = csv_path\n",
        "        self.update_freq = update_freq\n",
        "        # для анализа весов: список словарей с ключами run, epoch, peak_value, actual_freq, snr, pval, img_path\n",
        "        self.weight_analysis = []\n",
        "        if os.path.exists(csv_path):\n",
        "            try:\n",
        "                os.remove(csv_path)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    def log(self, epoch, acc, loss, run_name, seed, time_elapsed, **hparams):\n",
        "        rec = {\n",
        "            \"run\": run_name,\n",
        "            \"seed\": seed,\n",
        "            \"epoch\": int(epoch),\n",
        "            \"accuracy\": float(acc),\n",
        "            \"loss\": float(loss),\n",
        "            \"time\": float(time_elapsed),\n",
        "        }\n",
        "        rec.update(hparams)\n",
        "        self.history.append(rec)\n",
        "\n",
        "        if epoch % self.update_freq == 0:\n",
        "            pd.DataFrame(self.history).to_csv(self.csv_path, index=False)\n",
        "            self.plot()\n",
        "\n",
        "    def log_weight_analysis(self, rec: Dict):\n",
        "        \"\"\"Добавляем запись в weight_analysis (не в main history).\"\"\"\n",
        "        self.weight_analysis.append(rec)\n",
        "        # периодически сохраняем\n",
        "        if len(self.weight_analysis) % 5 == 0:\n",
        "            pd.DataFrame(self.weight_analysis).to_csv(\"weight_analysis.csv\", index=False)\n",
        "\n",
        "    def plot(self):\n",
        "        clear_output(wait=True)\n",
        "        df = pd.DataFrame(self.history)\n",
        "        plt.figure(figsize=(16, 10))\n",
        "\n",
        "        if not df.empty:\n",
        "            # точность по времени\n",
        "            plt.subplot(221)\n",
        "            for run in df[\"run\"].unique():\n",
        "                run_df = df[df[\"run\"] == run]\n",
        "                plt.plot(run_df[\"time\"], run_df[\"accuracy\"], label=run, marker='o', markersize=4)\n",
        "            plt.xlabel(\"Время (сек)\")\n",
        "            plt.ylabel(\"Точность (%)\")\n",
        "            plt.title(\"Скорость сходимости\")\n",
        "            plt.legend()\n",
        "            plt.grid(True)\n",
        "\n",
        "            # точность по эпохам\n",
        "            plt.subplot(222)\n",
        "            for run in df[\"run\"].unique():\n",
        "                run_df = df[df[\"run\"] == run]\n",
        "                plt.plot(run_df[\"epoch\"], run_df[\"accuracy\"], label=run)\n",
        "            plt.xlabel(\"Эпоха\")\n",
        "            plt.ylabel(\"Точность (%)\")\n",
        "            plt.title(\"Динамика обучения\")\n",
        "            plt.grid(True)\n",
        "\n",
        "        # Визуализация анализа весов (если есть данные)\n",
        "        if self.weight_analysis:\n",
        "            wadf = pd.DataFrame(self.weight_analysis)\n",
        "            # DSI сила (пик или SNR)\n",
        "            plt.subplot(223)\n",
        "            for run in wadf[\"run\"].unique():\n",
        "                run_df = wadf[wadf[\"run\"] == run]\n",
        "                plt.plot(run_df[\"epoch\"], run_df[\"peak_value\"], label=f\"{run} (peak)\")\n",
        "            plt.xlabel(\"Эпоха\")\n",
        "            plt.ylabel(\"Peak (FFT of hist)\")\n",
        "            plt.title(\"Сила лог-периодичности (DSI)\")\n",
        "            plt.grid(True)\n",
        "            plt.legend()\n",
        "\n",
        "            # частота\n",
        "            plt.subplot(224)\n",
        "            for run in wadf[\"run\"].unique():\n",
        "                run_df = wadf[wadf[\"run\"] == run]\n",
        "                plt.plot(run_df[\"epoch\"], run_df[\"actual_freq\"], label=run)\n",
        "            plt.axhline(y=1.0 / np.log(3.0), color='r', linestyle='--', alpha=0.6)\n",
        "            plt.xlabel(\"Эпоха\")\n",
        "            plt.ylabel(\"Частота (1 / ln(3) ≈ 0.9102)\")\n",
        "            plt.title(\"Частота лог-периодичности (теория: 1/ln(3))\")\n",
        "            plt.grid(True)\n",
        "            plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def final_report(self):\n",
        "        df = pd.DataFrame(self.history)\n",
        "        if df.empty:\n",
        "            print(\"Нет данных для отчёта.\")\n",
        "            return\n",
        "\n",
        "        target_acc = 55.0\n",
        "        results = {}\n",
        "\n",
        "        for run in df[\"run\"].unique():\n",
        "            run_df = df[df[\"run\"] == run]\n",
        "            final_accs = []\n",
        "            times_to_target = []\n",
        "\n",
        "            for seed in run_df[\"seed\"].unique():\n",
        "                seed_df = run_df[run_df[\"seed\"] == seed]\n",
        "                final_acc = seed_df.tail(3)[\"accuracy\"].mean()\n",
        "                final_accs.append(final_acc)\n",
        "\n",
        "                reached = seed_df[seed_df[\"accuracy\"] >= target_acc]\n",
        "                if not reached.empty:\n",
        "                    times_to_target.append(reached.iloc[0][\"time\"])\n",
        "\n",
        "            results[run] = {\n",
        "                \"final_acc_mean\": np.mean(final_accs),\n",
        "                \"final_acc_std\": np.std(final_accs),\n",
        "                \"time_mean\": np.mean(times_to_target) if times_to_target else None,\n",
        "                \"time_std\": np.std(times_to_target) if len(times_to_target) > 1 else 0.0\n",
        "            }\n",
        "\n",
        "        baseline_time = results.get(\"Baseline (L2)\", {}).get(\"time_mean\", None)\n",
        "\n",
        "        print(\"\\n===== РАСШИРЕННЫЙ ОТЧЁТ =====\")\n",
        "        for run, data in results.items():\n",
        "            acc_str = f\"{data['final_acc_mean']:.2f} ± {data['final_acc_std']:.2f}\"\n",
        "            if data[\"time_mean\"] is not None and baseline_time is not None:\n",
        "                if run != \"Baseline (L2)\":\n",
        "                    speedup = baseline_time / data[\"time_mean\"] if data[\"time_mean\"] > 0 else float('nan')\n",
        "                    speedup_str = f\"{speedup:.2f}x\"\n",
        "                else:\n",
        "                    speedup_str = \"-\"\n",
        "                time_str = f\"{data['time_mean']:.1f} ± {data['time_std']:.1f} сек\"\n",
        "            else:\n",
        "                speedup_str = \"-\"\n",
        "                time_str = \"не достигнуто\"\n",
        "            print(f\"{run:20} | Acc: {acc_str} | Время: {time_str} | Ускорение: {speedup_str}\")\n",
        "\n",
        "        if self.weight_analysis:\n",
        "            print(\"\\nСохранён анализ весов в weight_analysis.csv и изображения в weight_plots/\")\n",
        "\n",
        "# единый логгер\n",
        "LOGGER = AcceleratedLogger()\n",
        "\n",
        "# ========== 3) даталоадеры (с опциональными аугментациями) ==========\n",
        "class Cutout(object):\n",
        "    \"\"\"Простой Cutout как трансформ.\"\"\"\n",
        "    def __init__(self, size=8):\n",
        "        self.size = size\n",
        "\n",
        "    def __call__(self, img: torch.Tensor):\n",
        "        c, h, w = img.shape\n",
        "        sz = self.size\n",
        "        y = np.random.randint(h)\n",
        "        x = np.random.randint(w)\n",
        "        y1 = np.clip(y - sz // 2, 0, h)\n",
        "        y2 = np.clip(y + sz // 2, 0, h)\n",
        "        x1 = np.clip(x - sz // 2, 0, w)\n",
        "        x2 = np.clip(x + sz // 2, 0, w)\n",
        "        img[:, y1:y2, x1:x2] = 0.0\n",
        "        return img\n",
        "\n",
        "def get_fast_dataloaders(batch_size=128, use_randaugment=False, use_cutout=False):\n",
        "    mean = (0.5071, 0.4867, 0.4408)\n",
        "    std = (0.2675, 0.2565, 0.2761)\n",
        "\n",
        "    aug = []\n",
        "    if use_randaugment:\n",
        "        try:\n",
        "            aug.append(T.RandAugment(num_ops=2, magnitude=9))\n",
        "        except AttributeError:\n",
        "            pass\n",
        "\n",
        "    transform_train = T.Compose([\n",
        "        T.RandomCrop(32, padding=4),\n",
        "        T.RandomHorizontalFlip(),\n",
        "        *aug,\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean, std),\n",
        "        *([Cutout(size=8)] if use_cutout else []),\n",
        "    ])\n",
        "    transform_test = T.Compose([\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean, std)\n",
        "    ])\n",
        "\n",
        "    trainset = torchvision.datasets.CIFAR100(\"./data\", train=True, download=True, transform=transform_train)\n",
        "    testset = torchvision.datasets.CIFAR100(\"./data\", train=False, download=True, transform=transform_test)\n",
        "\n",
        "    persistent = NUM_WORKERS > 0\n",
        "    train_loader = DataLoader(\n",
        "        trainset, batch_size=batch_size, shuffle=True,\n",
        "        num_workers=NUM_WORKERS, pin_memory=True,\n",
        "        persistent_workers=persistent)\n",
        "    test_loader = DataLoader(\n",
        "        testset, batch_size=256, shuffle=False,\n",
        "        num_workers=NUM_WORKERS, pin_memory=True)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "def fast_mixup(x, y, alpha=0.2):\n",
        "    if alpha <= 0.0:\n",
        "        return x, y, y, 1.0\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    idx = torch.randperm(x.size(0), device=x.device)\n",
        "    mixed_x = lam * x + (1.0 - lam) * x[idx]\n",
        "    return mixed_x, y, y[idx], lam\n",
        "\n",
        "# ========== 4) FHR (безопасный) ==========\n",
        "class FHRRegularizer(nn.Module):\n",
        "    def __init__(self, model: nn.Module, a: float = 3.0, percentile: int = 15,\n",
        "                 kappa: float = 2.0, exclude_bn_bias: bool = True):\n",
        "        super().__init__()\n",
        "        assert a > 1.0\n",
        "        self.a = float(a)\n",
        "        self.log_a = math.log(self.a)\n",
        "        self.percentile = int(percentile)\n",
        "        self.kappa = float(kappa)\n",
        "        self.exclude_bn_bias = bool(exclude_bn_bias)\n",
        "        self.w0_by_param: Dict[str, float] = self._compute_w0_per_param(model)\n",
        "        self.unstable_ratio = 0.0\n",
        "        self.stability_history = []\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _compute_w0_per_param(self, model: nn.Module) -> Dict[str, float]:\n",
        "        w0: Dict[str, float] = {}\n",
        "        for name, p in model.named_parameters():\n",
        "            if not p.requires_grad:\n",
        "                continue\n",
        "            if self.exclude_bn_bias and (name.endswith('.bias') or 'bn' in name.lower() or 'downsample.1' in name.lower()):\n",
        "                continue\n",
        "            if p.numel() == 0:\n",
        "                continue\n",
        "            q = torch.quantile(p.detach().abs().flatten(), self.percentile / 100.0)\n",
        "            q = float(torch.clamp(q, min=1e-8).item())\n",
        "            w0[name] = q\n",
        "        return w0\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def refresh_w0(self, model: nn.Module):\n",
        "        self.w0_by_param = self._compute_w0_per_param(model)\n",
        "\n",
        "    def forward(self, model: nn.Module) -> torch.Tensor:\n",
        "        acc = None\n",
        "        two_pi_over_log_a = (2.0 * math.pi) / self.log_a\n",
        "        params_iter = list(model.parameters())\n",
        "        if len(params_iter) == 0:\n",
        "            return torch.tensor(0.0, device=DEVICE)\n",
        "\n",
        "        device = params_iter[0].device\n",
        "        param_count = 0\n",
        "        unstable_elements = 0\n",
        "        total_elements = 0\n",
        "\n",
        "        for name, p in model.named_parameters():\n",
        "            if not p.requires_grad:\n",
        "                continue\n",
        "            if self.exclude_bn_bias and (name.endswith('.bias') or 'bn' in name.lower() or 'downsample.1' in name.lower()):\n",
        "                continue\n",
        "            if p.numel() == 0:\n",
        "                continue\n",
        "\n",
        "            abs_w = p.abs()\n",
        "            w0 = self.w0_by_param.get(name, 1e-3)\n",
        "            log_vals = torch.log(abs_w / w0 + 1e-12)\n",
        "            theta = two_pi_over_log_a * log_vals\n",
        "\n",
        "            # безопасный tan: sin/cos с защитой от нуля\n",
        "            sin_theta = torch.sin(theta)\n",
        "            cos_theta = torch.cos(theta)\n",
        "            safe_cos = torch.clamp(cos_theta, min=1e-6)\n",
        "            stab = sin_theta / safe_cos\n",
        "\n",
        "            # stability mask на уровне элементов\n",
        "            stability_mask = (two_pi_over_log_a * stab.abs()) < 4.0\n",
        "            unstable_elements += ((~stability_mask).float().sum().item())\n",
        "            total_elements += stability_mask.numel()\n",
        "\n",
        "            # мягкий гейт\n",
        "            gate = torch.sigmoid(self.kappa * (4.0 - (two_pi_over_log_a * stab).abs()))\n",
        "\n",
        "            term = (abs_w ** 4) * 0.5 * (1.0 - torch.cos(theta)) * gate\n",
        "            layer_pen = term.mean()\n",
        "\n",
        "            acc = layer_pen if acc is None else acc + layer_pen\n",
        "            param_count += 1\n",
        "\n",
        "        self.unstable_ratio = unstable_elements / max(1, total_elements)\n",
        "        self.stability_history.append(self.unstable_ratio)\n",
        "\n",
        "        # Автонастройка kappa\n",
        "        if self.unstable_ratio > 0.3 and param_count > 0:\n",
        "            self.kappa = max(1.0, self.kappa * 0.95)\n",
        "\n",
        "        if acc is None or param_count == 0:\n",
        "            return torch.zeros((), device=device)\n",
        "\n",
        "        return acc / float(param_count)\n",
        "\n",
        "# ========== 5) оценка ==========\n",
        "@torch.no_grad()\n",
        "def evaluate_model(model, loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    for x, y in loader:\n",
        "        x = x.to(DEVICE, non_blocking=True)\n",
        "        y = y.to(DEVICE, non_blocking=True)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            logits = model(x)\n",
        "        pred = logits.argmax(1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += y.size(0)\n",
        "    return 100.0 * correct / max(1, total)\n",
        "\n",
        "# ========== 6) утилиты анализа DSI ==========\n",
        "def _fft_peak_stats_from_hist(hist: np.ndarray, bin_width: float, expected_freq: float,\n",
        "                              n_boot: int = 400) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Возвращает peak, snr, pval и actual_freq.\n",
        "    Используется rFFT от центрированной гистограммы с Hann-окном.\n",
        "    \"\"\"\n",
        "    h = hist - hist.mean()\n",
        "    window = np.hanning(len(h))\n",
        "    h_win = h * window\n",
        "    fft_vals = np.abs(np.fft.rfft(h_win))\n",
        "    freqs = np.fft.rfftfreq(len(h), d=bin_width)\n",
        "\n",
        "    idx = int(np.argmin(np.abs(freqs - expected_freq)))\n",
        "    peak = float(fft_vals[idx])\n",
        "    snr = peak / (np.median(fft_vals) + 1e-12)\n",
        "\n",
        "    # bootstrap/permutation нулевой модели\n",
        "    sims = []\n",
        "    for _ in range(max(1, n_boot)):\n",
        "        perm = np.random.permutation(h)\n",
        "        sim_fft = np.abs(np.fft.rfft(perm * window))\n",
        "        sims.append(sim_fft[idx])\n",
        "    sims = np.array(sims)\n",
        "    pval = float((np.sum(sims >= peak) + 1) / (len(sims) + 1))\n",
        "\n",
        "    return {\"peak\": peak, \"snr\": snr, \"pval\": pval, \"actual_freq\": float(freqs[idx])}\n",
        "\n",
        "def analyze_weight_distribution_fft_hist(model, epoch, run_name,\n",
        "                                         nbins=256, n_boot=400,\n",
        "                                         img_suffix=\"global\") -> Dict:\n",
        "    \"\"\"\n",
        "    Анализ DSI: гистограмма log(|w|) -> rFFT -> peak/SNR/p-value.\n",
        "    Сохраняет изображение гистограммы с маркерами в weight_plots/.\n",
        "    \"\"\"\n",
        "    all_weights: List[np.ndarray] = []\n",
        "    for p in model.parameters():\n",
        "        if p.requires_grad:\n",
        "            a = p.detach().abs().cpu().numpy().flatten()\n",
        "            if a.size:\n",
        "                # подвыборка больших массивов\n",
        "                if a.size > 200000:\n",
        "                    a = np.random.choice(a, size=200000, replace=False)\n",
        "                all_weights.append(a)\n",
        "    if not all_weights:\n",
        "        return {}\n",
        "\n",
        "    lw = np.log(np.concatenate(all_weights) + 1e-12)\n",
        "    hist, edges = np.histogram(lw, bins=nbins, density=True)\n",
        "    bin_width = edges[1] - edges[0]\n",
        "\n",
        "    expected_freq = 1.0 / np.log(3.0)  # период ln(3)\n",
        "    stats = _fft_peak_stats_from_hist(hist, bin_width, expected_freq, n_boot=n_boot)\n",
        "\n",
        "    # Сохраняем гистограмму как файл\n",
        "    centers = (edges[:-1] + edges[1:]) / 2.0\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(centers, hist, label='hist(log|w|)')\n",
        "    w0 = np.mean(lw)\n",
        "    for k in range(-5, 6):\n",
        "        peak_x = w0 + k * np.log(3.0)\n",
        "        if centers.min() <= peak_x <= centers.max():\n",
        "            plt.axvline(peak_x, color='r', linestyle='--', alpha=0.3)\n",
        "    plt.title(f\"hist(log|w|) — {run_name} epoch={epoch} [{img_suffix}]\")\n",
        "    plt.xlabel(\"log(|w|)\")\n",
        "    plt.ylabel(\"density\")\n",
        "    plt.grid(True)\n",
        "    fname = f\"weight_plots/{run_name.replace(' ', '_')}_epoch{epoch}_{img_suffix}.png\"\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(fname)\n",
        "    plt.close()\n",
        "\n",
        "    return {\n",
        "        \"run\": run_name,\n",
        "        \"epoch\": int(epoch),\n",
        "        \"dsi_peak\": float(stats[\"peak\"]),\n",
        "        \"expected_freq\": float(expected_freq),\n",
        "        \"actual_freq\": float(stats[\"actual_freq\"]),\n",
        "        \"peak_value\": float(stats[\"peak\"]),\n",
        "        \"snr\": float(stats[\"snr\"]),\n",
        "        \"pval\": float(stats[\"pval\"]),\n",
        "        \"img_path\": fname\n",
        "    }\n",
        "\n",
        "def analyze_layers_dsi(model, epoch, run_name,\n",
        "                       nbins=192, n_boot=200, topk=8) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    По-слойный DSI-анализ.\n",
        "    Возвращает путь к bar-графику top-K слоёв по SNR (и сохраняет CSV).\n",
        "    \"\"\"\n",
        "    expected_freq = 1.0 / np.log(3.0)\n",
        "    rows = []\n",
        "    for name, p in model.named_parameters():\n",
        "        if (not p.requires_grad) or p.numel() == 0:\n",
        "            continue\n",
        "        # игнорируем bias и BN/shortcut по умолчанию — можно при желании убрать\n",
        "        lname = name.lower()\n",
        "        if any(x in lname for x in ['.bias', 'bn', 'downsample.1']):\n",
        "            continue\n",
        "\n",
        "        w = p.detach().abs().cpu().numpy().flatten()\n",
        "        if w.size == 0:\n",
        "            continue\n",
        "        # подвыборка\n",
        "        if w.size > 150000:\n",
        "            w = np.random.choice(w, size=150000, replace=False)\n",
        "        lw = np.log(w + 1e-12)\n",
        "\n",
        "        hist, edges = np.histogram(lw, bins=nbins, density=True)\n",
        "        bin_width = edges[1] - edges[0]\n",
        "        stats = _fft_peak_stats_from_hist(hist, bin_width, expected_freq, n_boot=n_boot)\n",
        "        rows.append({\n",
        "            \"layer\": name,\n",
        "            \"peak\": float(stats[\"peak\"]),\n",
        "            \"snr\": float(stats[\"snr\"]),\n",
        "            \"pval\": float(stats[\"pval\"]),\n",
        "            \"actual_freq\": float(stats[\"actual_freq\"]),\n",
        "        })\n",
        "\n",
        "    if not rows:\n",
        "        return None\n",
        "\n",
        "    df = pd.DataFrame(rows).sort_values([\"snr\", \"peak\"], ascending=False)\n",
        "    csv_path = f\"weight_plots/{run_name.replace(' ', '_')}_epoch{epoch}_layer_dsi.csv\"\n",
        "    df.to_csv(csv_path, index=False)\n",
        "\n",
        "    # bar-плот top-K\n",
        "    top = df.head(topk)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.barh(top[\"layer\"][::-1], top[\"snr\"][::-1])\n",
        "    plt.xlabel(\"SNR (peak / median FFT)\")\n",
        "    plt.title(f\"Top-{topk} слоёв по SNR — {run_name} epoch={epoch}\")\n",
        "    plt.tight_layout()\n",
        "    fig_path = f\"weight_plots/{run_name.replace(' ', '_')}_epoch{epoch}_layer_top{topk}.png\"\n",
        "    plt.savefig(fig_path)\n",
        "    plt.close()\n",
        "    return fig_path\n",
        "\n",
        "# глобально текущее описание эксперимента (чтобы не менять сигнатуры)\n",
        "_CURRENT_EXP = {}\n",
        "\n",
        "# ========== 7) обучение ==========\n",
        "def build_optimizer_and_scheduler(model, epochs, exp):\n",
        "    opt_name = exp.get(\"opt\", \"adamw\").lower()\n",
        "    lr = float(exp.get(\"lr\", 0.001 if opt_name == \"adamw\" else 0.1))\n",
        "    weight_decay = float(exp.get(\"weight_decay\", 0.01))\n",
        "    warmup_epochs = int(exp.get(\"warmup_epochs\", 3 if opt_name == \"sgd\" else 0))\n",
        "\n",
        "    if opt_name == \"sgd\":\n",
        "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay, nesterov=True)\n",
        "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "    else:\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "        scheduler = None\n",
        "\n",
        "    return optimizer, scheduler, warmup_epochs\n",
        "\n",
        "def maybe_warmup(optimizer, base_lr, epoch, warmup_epochs):\n",
        "    if warmup_epochs > 0 and epoch <= warmup_epochs:\n",
        "        scale = float(epoch) / float(max(1, warmup_epochs))\n",
        "        for pg in optimizer.param_groups:\n",
        "            pg[\"lr\"] = base_lr * scale\n",
        "\n",
        "def train_experiment(run_name, use_fhr=False, use_mixup=False, epochs=30, seed=42, **exp_overrides):\n",
        "    setup_accelerated_environment(seed)\n",
        "    start_time = time.time()\n",
        "\n",
        "    exp = {**_CURRENT_EXP, **exp_overrides} if isinstance(_CURRENT_EXP, dict) else exp_overrides\n",
        "    batch_size = int(exp.get(\"batch_size\", 128))\n",
        "    use_ra = bool(exp.get(\"use_randaugment\", False))\n",
        "    use_cutout = bool(exp.get(\"use_cutout\", False))\n",
        "    mix_alpha = float(exp.get(\"mix_alpha\", 0.2))\n",
        "    grad_clip = float(exp.get(\"grad_clip\", 1.0))\n",
        "    fhr_refresh_every = int(exp.get(\"fhr_refresh_every\", 5))\n",
        "    fhr_ramp_epochs = int(exp.get(\"fhr_ramp_epochs\", 5))\n",
        "    early_stop_patience = int(exp.get(\"early_stop_patience\", 8))\n",
        "    analyze_every = int(exp.get(\"analyze_every\", 5))  # анализ DSI каждые N эпох\n",
        "    nbins = int(exp.get(\"nbins\", 256))\n",
        "    n_boot = int(exp.get(\"n_boot\", 400))\n",
        "    layer_analysis = bool(exp.get(\"layer_analysis\", True))\n",
        "    layer_topk = int(exp.get(\"layer_topk\", 8))\n",
        "\n",
        "    train_loader, test_loader = get_fast_dataloaders(batch_size=batch_size, use_randaugment=use_ra, use_cutout=use_cutout)\n",
        "    model = torchvision.models.resnet18(num_classes=100).to(DEVICE)\n",
        "\n",
        "    optimizer, scheduler, warmup_epochs = build_optimizer_and_scheduler(model, epochs, exp)\n",
        "    base_lr = optimizer.param_groups[0][\"lr\"]\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=float(exp.get(\"label_smoothing\", 0.0)))\n",
        "\n",
        "    if use_fhr:\n",
        "        fhr = FHRRegularizer(\n",
        "            model,\n",
        "            a=float(exp.get(\"a\", 3.0)),\n",
        "            percentile=int(exp.get(\"percentile\", 15)),\n",
        "            kappa=float(exp.get(\"kappa\", 2.0)),\n",
        "            exclude_bn_bias=bool(exp.get(\"exclude_bn_bias\", True)),\n",
        "        )\n",
        "        fhr_weight = float(exp.get(\"fhr_weight\", 0.05))\n",
        "    else:\n",
        "        fhr, fhr_weight = None, 0.0\n",
        "\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    # early stopping & checkpoint\n",
        "    best_acc = 0.0\n",
        "    patience_counter = 0\n",
        "    stopped_early = False\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        maybe_warmup(optimizer, base_lr, epoch, warmup_epochs)\n",
        "\n",
        "        total_loss = 0.0\n",
        "        progress = tqdm(train_loader, desc=f\"{run_name} Epoch {epoch}/{epochs}\")\n",
        "\n",
        "        curr_fhr_weight = fhr_weight * min(1.0, float(epoch) / max(1.0, fhr_ramp_epochs))\n",
        "\n",
        "        for x, y in progress:\n",
        "            x = x.to(DEVICE, non_blocking=True)\n",
        "            y = y.to(DEVICE, non_blocking=True)\n",
        "\n",
        "            if use_mixup:\n",
        "                x, ya, yb, lam = fast_mixup(x, y, alpha=mix_alpha)\n",
        "            else:\n",
        "                ya, yb, lam = y, y, 1.0\n",
        "\n",
        "            with torch.cuda.amp.autocast():\n",
        "                logits = model(x)\n",
        "                loss_ce = lam * criterion(logits, ya) + (1.0 - lam) * criterion(logits, yb)\n",
        "                loss = loss_ce + (curr_fhr_weight * fhr(model) if fhr is not None else 0.0)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            # unscale -> clip -> NaN/Inf check\n",
        "            scaler.unscale_(optimizer)\n",
        "            if grad_clip is not None and grad_clip > 0.0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip)\n",
        "\n",
        "            nan_inf = False\n",
        "            for p in model.parameters():\n",
        "                if p.grad is not None:\n",
        "                    g = p.grad\n",
        "                    if torch.isnan(g).any() or torch.isinf(g).any():\n",
        "                        nan_inf = True\n",
        "                        break\n",
        "\n",
        "            if nan_inf:\n",
        "                print(f\"[WARNING] NaN/Inf grads at epoch {epoch}. Skipping step.\")\n",
        "                if fhr is not None:\n",
        "                    old_kappa = fhr.kappa\n",
        "                    fhr.kappa = max(1.0, fhr.kappa * 0.9)\n",
        "                    print(f\"  reduced fhr.kappa: {old_kappa:.3f} -> {fhr.kappa:.3f}\")\n",
        "                optimizer.zero_grad()\n",
        "                try:\n",
        "                    scaler.update()\n",
        "                except Exception:\n",
        "                    pass\n",
        "                continue\n",
        "\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            total_loss += float(loss.item())\n",
        "            progress.set_postfix({\"loss\": float(loss.item())})\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        # периодически обновляем w0 для FHR\n",
        "        if fhr is not None and (epoch % max(1, fhr_refresh_every) == 0):\n",
        "            fhr.refresh_w0(model)\n",
        "\n",
        "        acc = evaluate_model(model, test_loader)\n",
        "        total_time = time.time() - start_time\n",
        "\n",
        "        # Анализ распределения весов для проверки DSI (редко)\n",
        "        if fhr is not None and (epoch % analyze_every == 0 or epoch == epochs):\n",
        "            res = analyze_weight_distribution_fft_hist(\n",
        "                model, epoch, run_name, nbins=nbins, n_boot=n_boot, img_suffix=\"global\"\n",
        "            )\n",
        "            if res:\n",
        "                LOGGER.log_weight_analysis(res)\n",
        "\n",
        "            if layer_analysis:\n",
        "                try:\n",
        "                    layer_fig = analyze_layers_dsi(\n",
        "                        model, epoch, run_name, nbins=max(64, nbins//2), n_boot=max(50, n_boot//4), topk=layer_topk\n",
        "                    )\n",
        "                    if layer_fig:\n",
        "                        # фиктивная запись, чтобы видеть файл в weight_analysis.csv (не влияет на кривые)\n",
        "                        LOGGER.log_weight_analysis({\n",
        "                            \"run\": run_name, \"epoch\": int(epoch),\n",
        "                            \"peak_value\": res.get(\"peak_value\", np.nan) if res else np.nan,\n",
        "                            \"actual_freq\": res.get(\"actual_freq\", np.nan) if res else np.nan,\n",
        "                            \"snr\": res.get(\"snr\", np.nan) if res else np.nan,\n",
        "                            \"pval\": res.get(\"pval\", np.nan) if res else np.nan,\n",
        "                            \"img_path\": layer_fig, \"note\": \"layer_topk_plot\"\n",
        "                        })\n",
        "                except Exception as e:\n",
        "                    print(\"Layer DSI analysis failed:\", e)\n",
        "\n",
        "        # чекпоинтинг best model\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            patience_counter = 0\n",
        "            chk_name = f\"checkpoints/{run_name.replace(' ', '_')}_seed{seed}_best.pth\"\n",
        "            try:\n",
        "                torch.save({\n",
        "                    \"model_state\": model.state_dict(),\n",
        "                    \"optimizer_state\": optimizer.state_dict(),\n",
        "                    \"epoch\": epoch,\n",
        "                    \"acc\": acc\n",
        "                }, chk_name)\n",
        "            except Exception as e:\n",
        "                print(\"Warning: cannot save checkpoint:\", e)\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # логируем ключевые гиперпараметры\n",
        "        LOGGER.log(\n",
        "            epoch, acc, total_loss / max(1, len(train_loader)),\n",
        "            run_name, seed, total_time,\n",
        "            lr=optimizer.param_groups[0][\"lr\"],\n",
        "            fhr_weight=curr_fhr_weight,\n",
        "            kappa=(getattr(fhr, \"kappa\", None) if fhr is not None else None),\n",
        "            unstable_ratio=(getattr(fhr, \"unstable_ratio\", None) if fhr is not None else None),\n",
        "            percentile=(getattr(fhr, \"percentile\", None) if fhr is not None else None),\n",
        "            opt=type(optimizer).__name__,\n",
        "            mix_alpha=(mix_alpha if use_mixup else 0.0),\n",
        "            aug_ra=use_ra, aug_cutout=use_cutout,\n",
        "        )\n",
        "\n",
        "        # Early stopping\n",
        "        if patience_counter >= early_stop_patience and epoch > 15:\n",
        "            print(f\"Early stopping на эпохе {epoch}\")\n",
        "            stopped_early = True\n",
        "            break\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    if stopped_early:\n",
        "        print(f\"Обучение остановлено досрочно на эпохе {epoch} с точностью {best_acc:.2f}%\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# ========== 8) запуск ==========\n",
        "EXPERIMENTS = [\n",
        "    {\"name\": \"Baseline (L2)\", \"use_fhr\": False, \"use_mixup\": False,\n",
        "     \"opt\": \"adamw\", \"lr\": 0.001, \"weight_decay\": 0.01,\n",
        "     \"grad_clip\": 1.0},\n",
        "\n",
        "    {\"name\": \"MixUp\", \"use_fhr\": False, \"use_mixup\": True,\n",
        "     \"opt\": \"adamw\", \"lr\": 0.001, \"weight_decay\": 0.01,\n",
        "     \"mix_alpha\": 0.2, \"grad_clip\": 1.0},\n",
        "\n",
        "    {\"name\": \"FHR\", \"use_fhr\": True, \"use_mixup\": False,\n",
        "     \"opt\": \"sgd\", \"lr\": 0.1, \"weight_decay\": 0.0,\n",
        "     \"fhr_weight\": 0.05, \"a\": 3.0, \"percentile\": 15, \"kappa\": 2.0,\n",
        "     \"fhr_ramp_epochs\": 5, \"fhr_refresh_every\": 5, \"grad_clip\": 1.0,\n",
        "     \"warmup_epochs\": 3, \"label_smoothing\": 0.0,\n",
        "     \"use_randaugment\": False, \"use_cutout\": False,\n",
        "     \"analyze_every\": 5, \"nbins\": 256, \"n_boot\": 400,\n",
        "     \"layer_analysis\": True, \"layer_topk\": 8},\n",
        "\n",
        "    {\"name\": \"FHR+MixUp\", \"use_fhr\": True, \"use_mixup\": True,\n",
        "     \"opt\": \"sgd\", \"lr\": 0.1, \"weight_decay\": 0.0,\n",
        "     \"fhr_weight\": 0.05, \"a\": 3.0, \"percentile\": 15, \"kappa\": 2.0,\n",
        "     \"fhr_ramp_epochs\": 5, \"fhr_refresh_every\": 5, \"grad_clip\": 1.0,\n",
        "     \"warmup_epochs\": 3, \"mix_alpha\": 0.2, \"label_smoothing\": 0.0,\n",
        "     \"use_randaugment\": False, \"use_cutout\": False,\n",
        "     \"analyze_every\": 5, \"nbins\": 256, \"n_boot\": 400,\n",
        "     \"layer_analysis\": True, \"layer_topk\": 8},\n",
        "]\n",
        "\n",
        "SEEDS = [1337, 2023]\n",
        "\n",
        "print(\"===== ЗАПУСК ЭКСПЕРИМЕНТОВ =====\")\n",
        "for exp in EXPERIMENTS:\n",
        "    for seed in SEEDS:\n",
        "        print(f\"Запуск: {exp['name']} (seed={seed})\")\n",
        "        _CURRENT_EXP = exp\n",
        "        train_experiment(\n",
        "            run_name=exp[\"name\"],\n",
        "            use_fhr=exp[\"use_fhr\"],\n",
        "            use_mixup=exp[\"use_mixup\"],\n",
        "            epochs=int(exp.get(\"epochs\", 30)),\n",
        "            seed=seed\n",
        "        )\n",
        "\n",
        "LOGGER.final_report()\n",
        "print(\"Результаты сохранены в:\", os.path.abspath(\"fhr_results.csv\"))\n",
        "print(\"Анализ весов (если был) сохранён в:\", os.path.abspath(\"weight_analysis.csv\"))\n",
        "print(\"Изображения распределений в:\", os.path.abspath(\"weight_plots/\"))\n",
        "print(\"Чекпоинты в:\", os.path.abspath(\"checkpoints/\"))"
      ]
    }
  ]
}